# -*- coding: utf-8 -*-
"""Breast_Cancer_EfficientNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IWd5DBEqrtbb2SS9k1kIGdIgKga_2FEg
"""

! pip install opendatasets

#===============================
# Step-1: import libraries
#===============================


import warnings
warnings.filterwarnings('ignore')
import opendatasets as od

import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Dropout,GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

## {"username":"chanchalsaha7","key":"9ae140efdb070deb48f1b8effc11f3d5"}

link='https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images'
od.download(link)

#====================================================
#Step-2: load Dataset and Create a Dataframe
#===================================================

base_dir='/content/breast-histopathology-images'

pattern=os.path.join(base_dir,'**',"*.png")

filepaths=glob.glob(pattern,recursive=True)

label=[]

for file in filepaths:
  label.append(os.path.basename(os.path.dirname(file)))

df=pd.DataFrame({
    'path':filepaths,
    'label':label
})
df.head()

# =====================================================
# Step-3: Train-test-split
#=====================================================
train_df,valtest_df=train_test_split(df,test_size=0.3,random_state=42,stratify=df['label'])
val_df,test_df=train_test_split(valtest_df,test_size=0.5,random_state=42,stratify=valtest_df['label'])

print(train_df.shape)
print(test_df.shape)
print(val_df.shape)

# =====================================================
# Step-4: Data Generators
# =====================================================
Datagen=ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

Datagen2=ImageDataGenerator(
    rescale=1./255
)

IMG_SIZE=(50,50)
BATCH_SIZE=16
EPOCHS=5

train_gen=Datagen.flow_from_dataframe(
    train_df,
    x_col='path',
    y_col='label',
    batch_size=BATCH_SIZE,
    seed=42,
    target_size=IMG_SIZE,
    shuffle=True,
    class_mode='binary'
)

val_gen=Datagen.flow_from_dataframe(
    val_df,
    x_col='path',
    y_col='label',
    batch_size=BATCH_SIZE,
    seed=42,
    target_size=IMG_SIZE,
    shuffle=False,
    class_mode='binary'
)

test_gen=Datagen.flow_from_dataframe(
    test_df,
    x_col='path',
    y_col='label',
    batch_size=BATCH_SIZE,
    seed=42,
    target_size=IMG_SIZE,
    shuffle=False,
    class_mode='binary'
)

# =====================================================
# Step-5: Transfer Learning Model
# =====================================================
base_model=EfficientNetB0(include_top=False,weights='imagenet',input_shape=IMG_SIZE+(3,))



base_model.trainable=False

model=Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(1,activation='sigmoid')
])

model.compile(
    optimizer=Adam(1e-3),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping

checkpoint_cb=ModelCheckpoint('bestmodel.h5',save_best_only=True)
earlystop_cb=EarlyStopping(
    patience=3,
    restore_best_weights=True
)

# =====================================================
# Step-6: Train Frozen Model (Baseline)
# =====================================================

history=model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[checkpoint_cb,earlystop_cb]
)

#===================================
# Step-7: Evaluation
#===================================
loss,acc =model.evaluate(test_gen)
print(f'Test Accuracy: {acc*100:.2f}%')

from sklearn.metrics import confusion_matrix,classification_report

# Confusion matrix
y_true=test_gen.classes
y_pred=(model.predict(test_gen)>0.5).astype('int32').flatten()

cm=confusion_matrix(y_true,y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(cm,annot=True,fmt='d',xticklabels=['Non_IDC','IDC'],yticklabels=['Non_IDC','IDC'])
plt.xlabel('predicted')
plt.ylabel('True')
plt.show()


 # classification report
print(classification_report(y_true,y_pred,target_names=['Non_IDC','IDC']))

# =====================================================
# Step-8: Fine Tuning (Unfreeze last layers)
# =====================================================
from tensorflow.keras.callbacks import ReduceLROnPlateau

base_model.trainable=True

# fine-tune only last 50 layers

for layer in base_model.layers[:-50]:
  layer.trainable=False

model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

fine_tune_callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.5, patience=3),
    ModelCheckpoint("EfficientNetB0_finetuned.h5", save_best_only=True)
]

from sklearn.utils import class_weight
import numpy as np

y_train = train_gen.classes
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))
print("Class Weights:", class_weights)

h2=model.fit(
    train_gen,
    validation_data=val_gen,
    callbacks=fine_tune_callbacks,
    epochs=10,
    class_weight=class_weights
)

#========================================
# Step-9: Fine Tunning Model Evaluation
#========================================
loss,acc =model.evaluate(test_gen)
print(f'Test Accuracy: {acc*100:.2f}%')

 # Confusion matrix
y_true=test_gen.classes
y_pred=(model.predict(test_gen)>0.5).astype('int32').flatten()

cm=confusion_matrix(y_true,y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(cm,annot=True,fmt='d',xticklabels=['Non_IDC','IDC'],yticklabels=['Non_IDC','IDC'])
plt.xlabel('predicted')
plt.ylabel('True')
plt.show()


 # classification report
print(classification_report(y_true,y_pred,target_names=['Non_IDC','IDC']))

# Training & Validation Accuracy
plt.figure(figsize=(8,4))
plt.plot(h2.history['accuracy'], label='Train Accuracy')
plt.plot(h2.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.show()

# Training & Validation Loss
plt.figure(figsize=(8,4))
plt.plot(h2.history['loss'], label='Train Loss')
plt.plot(h2.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.show()

